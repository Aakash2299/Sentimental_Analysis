# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XzEWyxbcO9fsSnf-EpdTc6HOOCJ2rNK8
"""

import re
import nltk
import string
import pandas as pd
import numpy  as np
import urllib.request
import requests
from xlrd import XLRDError
from nltk.tokenize import RegexpTokenizer, sent_tokenize
nltk.download('punkt')


org_file         = pd.read_excel("cik_list.xlsx", header=None) 
header           = org_file.iloc[0,:]                           
org_file.columns = header                                      
secfname_or      = org_file.copy()                             
secfname_or      = secfname_or.iloc[1:,5]                    
file_rec         = org_file.iloc[1:,:]                     

pd.options.mode.chained_assignment = None  
link = 'https://www.sec.gov/Archives/'
file_rec['SECFNAME'] = link + file_rec['SECFNAME'].astype(str)
#file_rec.head(5)

new_columns = ['mda_positive_score', 'mda_negative_score', 'mda_polarity_score', 'mda_average_sentence_length', 'mda_percentage_of_complex_words', 'mda_fog_index',
               'mda_complex_word_count', 'mda_word_count', 'mda_uncertainty_score', 'mda_constraining_score', 'mda_positive_word_proportion', 'mda_negative_word_proportion',
               'mda_uncertainty_word_proportion', 'mda_constraining_word_proportion','qqdmr_positive_score','qqdmr_negative_score',
               'qqdmr_polarity_score','qqdmr_average_sentence_length','qqdmr_percentage_of_complex_words', 'qqdmr_fog_index','qqdmr_complex_word_count',
               'qqdmr_word_count', 'qqdmr_uncertainty_score', 'qqdmr_constraining_score', 'qqdmr_positive_word_proportion', 'qqdmr_negative_word_proportion',
               'qqdmr_uncertainty_word_proportion', 'qqdmr_constraining_word_proportion', 'rf_positive_score', 'rf_negative_score',
               'rf_polarity_score', 'rf_average_sentence_length', 'rf_percentage_of_complex_words', 'rf_fog_index', 'rf_complex_word_count',
               'rf_word_count', 'rf_uncertainty_score', 'rf_constraining_score', 'rf_positive_word_proportion', 'rf_negative_word_proportion',
               'rf_uncertainty_word_proportion', 'rf_constraining_word_proportion', 'constraining_words_whole_report']

file_rec = pd.concat([file_rec,pd.DataFrame(columns=new_columns)]) 
file_rec = file_rec.reset_index()                                  
del file_rec['index']                                

with open('StopWords_GenericLong.txt') as f:
    stop_words = list(f) 
    stop_words = [s.rstrip() for s in stop_words]
    stop_words = [x.lower() for x in stop_words]

positive = pd.read_excel("LoughranMcDonald_MasterDictionary_2018.xlsx", header=None)
negative = pd.read_excel("LoughranMcDonald_MasterDictionary_2018.xlsx", header=None)

positive = positive[0]
negative = negative[0]

positive_cleaned = list(set(positive)-set(stop_words)) 
negative_cleaned = list(set(negative)-set(stop_words)) 

positive_cleaned = [x for x in positive_cleaned]
negative_cleaned = [x for x in negative_cleaned]

uncertainity_dict = pd.read_excel("uncertainty_dictionary.xlsx", header=None)
constraining_dict = pd.read_excel("constraining_dictionary.xlsx", header=None)

uncertainity_words = uncertainity_dict[0].tolist()
uncertainity_words = [x.lower() for x in uncertainity_words]

constraining_words = constraining_dict[0].tolist()
constraining_words = [x.lower() for x in constraining_words]

file_rec.head(5)

for i in range(len(file_rec)):                   
    url = file_rec.iloc[i,5] 
    data = requests.request('get',url).content 
    data = [str(s, 'utf-8') for s in data if type(s)== bytes]         
    data = [s.strip('\n') for s in data]            
    data = list(filter(None, data))   

    data = " ".join(data)
    data = re.sub("(?is)<table[^>]*>(.*?)<\/table>", "", data)

    html_regex = re.compile(r'<.*?>')
    data = re.sub(html_regex,'',data)
    data = data.replace('&nbsp;','')
    input_data = re.sub(r'&#\d+;', '', data)   
    constr_whole_count = constr_whole_report(input_data)
    file_rec.loc[i, 'constraining_words_whole_report'] = constr_whole_count

    topic1_data, topic2_data, topic3_data = fetch_sections_data(input_data)

    topic1_data, topic2_data, topic3_data = data_cleaning(topic1_data, topic2_data, topic3_data)   

    if (topic1_data != 0):
    
        pos_score1, neg_score1, pol_score1 = pos_neg_pol(topic1_data)

        file_rec.loc[i, 'mda_positive_score'] = pos_score1
        file_rec.loc[i, 'mda_negative_score'] = neg_score1 
        file_rec.loc[i, 'mda_polarity_score'] = pol_score1

        avg_sent_len_val1 = avg_sent_len(topic1_data)
        file_rec.loc[i, 'mda_average_sentence_length'] = avg_sent_len_val1

        complex_count = 0
        word_count1, complex_count1, perc_complex_words1 = complex_word_count(topic1_data, complex_count)
        file_rec.loc[i, 'mda_word_count']                  = word_count1
        file_rec.loc[i, 'mda_complex_word_count']          = complex_count1
        file_rec.loc[i, 'mda_percentage_of_complex_words'] = perc_complex_words1

        fog_index1 = 0.4 * (avg_sent_len_val1 + perc_complex_words1)
        file_rec.loc[i, 'mda_fog_index'] = fog_index1

        uncer_score1, constr_score1 = uncer_constr_calc(topic1_data)
        perc_uncer1  = uncer_score1/word_count1
        perc_constr1 = constr_score1/word_count1
        file_rec.loc[i, 'mda_uncertainty_score']            = uncer_score1
        file_rec.loc[i, 'mda_constraining_score']           = constr_score1

        file_rec.loc[i, 'mda_uncertainty_word_proportion']  = perc_uncer1
        file_rec.loc[i, 'mda_constraining_word_proportion'] = perc_constr1

        perc_positive1 = pos_score1/word_count1
        perc_negative1 = neg_score1/word_count1
        file_rec.loc[i, 'mda_positive_word_proportion']  = perc_positive1
        file_rec.loc[i, 'mda_negative_word_proportion']  = perc_negative1

    else:
        file_rec.loc[i, 6:21] = 0

    if (topic2_data != 0):  

        pos_score2, neg_score2, pol_score2 = pos_neg_pol(topic2_data)
        file_rec.loc[i, 'qqdmr_positive_score'] = pos_score2
        file_rec.loc[i, 'qqdmr_negative_score'] = neg_score2 
        file_rec.loc[i, 'qqdmr_polarity_score'] = pol_score2

        avg_sent_len_val2 = avg_sent_len(topic2_data)
        file_rec.loc[i, 'qqdmr_average_sentence_length'] = avg_sent_len_val2

        complex_count = 0
        word_count2, complex_count2, perc_complex_words2 = complex_word_count(topic2_data, complex_count)
        file_rec.loc[i, 'qqdmr_word_count']                  = word_count2
        file_rec.loc[i, 'qqdmr_complex_word_count']          = complex_count2
        file_rec.loc[i, 'qqdmr_percentage_of_complex_words'] = perc_complex_words2

        fog_index2 = 0.4 * (avg_sent_len_val2 + perc_complex_words2)
        file_rec.loc[i, 'qqdmr_fog_index'] = fog_index2

        uncer_score2, constr_score2 = uncer_constr_calc(topic2_data)
        perc_uncer2  = uncer_score2/word_count2
        perc_constr2 = constr_score2/word_count2
        file_rec.loc[i, 'qqdmr_uncertainty_score']            = uncer_score2
        file_rec.loc[i, 'qqdmr_constraining_score']           = constr_score2

        file_rec.loc[i, 'qqdmr_uncertainty_word_proportion']  = perc_uncer2
        file_rec.loc[i, 'qqdmr_constraining_word_proportion'] = perc_constr2

        perc_positive2 = pos_score2/word_count2
        perc_negative2 = neg_score2/word_count2
        file_rec.loc[i, 'qqdmr_positive_word_proportion']  = perc_positive2
        file_rec.loc[i, 'qqdmr_negative_word_proportion']  = perc_negative2

    else:
        file_rec.loc[i, 20:33] = 0

    if (topic3_data != 0):  
    
        pos_score3, neg_score3, pol_score3 = pos_neg_pol(topic3_data)
        file_rec.loc[i, 'rf_positive_score'] = pos_score3
        file_rec.loc[i, 'rf_negative_score'] = neg_score3 
        file_rec.loc[i, 'rf_polarity_score'] = pol_score3

        avg_sent_len_val3 = avg_sent_len(topic3_data)
        file_rec.loc[i, 'rf_average_sentence_length'] = avg_sent_len_val3

        complex_count = 0
        word_count3, complex_count3, perc_complex_words3 = complex_word_count(topic3_data, complex_count)
        file_rec.loc[i, 'rf_word_count']                  = word_count3
        file_rec.loc[i, 'rf_complex_word_count']          = complex_count3
        file_rec.loc[i, 'rf_percentage_of_complex_words'] = perc_complex_words3

        fog_index3 = 0.4 * (avg_sent_len_val3 + perc_complex_words3)
        file_rec.loc[i, 'rf_fog_index'] = fog_index3

        uncer_score3, constr_score3 = uncer_constr_calc(topic3_data)
        perc_uncer3  = uncer_score3/word_count3
        perc_constr3 = constr_score3/word_count3
        file_rec.loc[i, 'rf_uncertainty_score']            = uncer_score3
        file_rec.loc[i, 'rf_constraining_score']           = constr_score3
        file_rec.loc[i, 'rf_uncertainty_word_proportion']  = perc_uncer3
        file_rec.loc[i, 'rf_constraining_word_proportion'] = perc_constr3

        perc_positive3 = pos_score3/word_count3
        perc_negative3 = neg_score3/word_count3
        file_rec.loc[i, 'rf_positive_word_proportion']  = perc_positive3
        file_rec.loc[i, 'rf_negative_word_proportion']  = perc_negative3

    else:
        file_rec.loc[i, 34:47] = 0

    pos_score1 = neg_score1 = pol_score1 = avg_sent_len_val1 = word_count1 = complex_count1 = perc_complex_words1 = fog_index1 = uncer_score1 = constr_score1 = perc_uncer1 = perc_constr1 = perc_positive1 = perc_negative1 = 0
    pos_score2 = neg_score2 = pol_score2 = avg_sent_len_val2 = word_count2 = complex_count2 = perc_complex_words2 = fog_index2 = uncer_score2 = constr_score2 = perc_uncer2 = perc_constr2 = perc_positive2 = perc_negative2 = 0
    pos_score3 = neg_score3 = pol_score3 = avg_sent_len_val3 = word_count3 = complex_count3 = perc_complex_words3 = fog_index3 = uncer_score3 = constr_score3 = perc_uncer3 = perc_constr3 = perc_positive3 = perc_negative3 = 0
    constr_whole_count = 0


           
def constr_whole_report(x):
    x      = x.translate(str.maketrans('','',string.punctuation)) 
    tokens = nltk.word_tokenize(x)                                
    tokens = [x.lower() for x in tokens]                          

    constr_whole_words = 0
    for word in tokens:                                         
        if word in constraining_words:                              
            constr_whole_words += 1     

        return constr_whole_words
    

def fetch_sections_data(data):
    topic1 = "MANAGEMENT'S DISCUSSION AND ANALYSIS"
    topic1_start = [m.start() for m in re.finditer(topic1, data)]

    if (len(topic1_start) == 0):
        topic1 = "MANAGEMENTS DISCUSSION AND ANALYSIS"
        topic1_start = [m.start() for m in re.finditer(topic1, data)]


    if (len(topic1_start) != 0):
        #topic_1_data has the data from the point where topic1 begins
        topic_1_data = data[topic1_start[0]:]

        item = "ITEM"
        topic1_end = [m.start() for m in re.finditer(item, topic_1_data)]

        if (len(topic1_end) == 0):
            topic_1_data = topic_1_data[0:]
        else:  
            topic_1_data = topic_1_data[0:topic1_end[0]-1]
    
    if (len(topic1_start) == 0):
        topic_1_data = 0  

    topic2 = "QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK"
    topic2_start = [m.start() for m in re.finditer(topic2, data)]

    if (len(topic2_start) != 0):

        topic_2_data = data[topic2_start[0]:]

        item = "ITEM"
        topic2_end = [m.start() for m in re.finditer(item, topic_2_data)]
        if (len(topic2_end) == 0):
            topic_2_data = topic_2_data[0:]
        else:  
            topic_2_data = topic_2_data[0:topic2_end[0]-1]     

    if (len(topic2_start) == 0):
        topic_2_data = 0 
        
    topic3 = "RISK FACTORS"
    topic3_start = [m.start() for m in re.finditer(topic3, data)]

    if (len(topic3_start) != 0):

        topic_3_data = data[topic3_start[0]:]

        item = "ITEM"
        topic3_end = [m.start() for m in re.finditer(item, topic_3_data)]

        if (len(topic3_end) == 0):
            topic_3_data = topic_3_data[0:]
        else:  
            topic_3_data = topic_3_data[0:topic3_end[0]-1]    

    if (len(topic3_start) == 0):
        topic_3_data = 0         

    return topic_1_data, topic_2_data, topic_3_data

def data_cleaning(x1, x2, x3):
    if (x1 != 0):
        x1 = nltk.word_tokenize(x1)                                     
        x1 = [word for word in x1 if word.lower() not in stop_words]  
        x1 = ' '.join(x1)                                             
    if (x2 != 0):
        x2 = nltk.word_tokenize(x2)                                   
        x2 = [word for word in x2 if word.lower() not in stop_words]  
        x2 = ' '.join(x2)                                                                       


    if (x3 != 0):
        x3 = nltk.word_tokenize(x3)                                   
        x3 = [word for word in x3 if word.lower() not in stop_words]  
        x3 = ' '.join(x3)                                             
    return x1, x2, x3

def pos_neg_pol(x):
    x      = x.translate(str.maketrans('','',string.punctuation)) 
    tokens = nltk.word_tokenize(x)                                
    tokens = [x.lower() for x in tokens]                          

    pos_words = neg_words = 0

    for word in tokens:
        if word in positive_cleaned:
            pos_words += 1
        if word in negative_cleaned:
            neg_words -= 1 

    pos_len = pos_words
    neg_len = neg_words * -1

    pol_score = (pos_len - neg_len)/((pos_len + neg_len) + 0.000001)

    return pos_len, neg_len, pol_score

def avg_sent_len(x):
    no_sent = len(sent_tokenize(x))                               
    x      = x.translate(str.maketrans('','',string.punctuation)) 
    tokens = nltk.word_tokenize(x)                                
    no_words = len(tokens)                                        

    if (no_sent != 0):
        avg_sent_len = round(no_words/no_sent)                      
    else:
        avg_sent_len = 0

    return avg_sent_len

def complex_word_count(word, complex_count):
    word   = word.translate(str.maketrans('','',string.punctuation)) 
    tokens = nltk.word_tokenize(word)                                
    no_words = len(tokens)                                           

    complex_count = 0                                                
    for word in tokens:
        word = word.lower()                                            
        vowels = "aeiou"    
        if (word.endswith(("es", "ed"))):                              
            count = 0
        else:
            count = 0
        for c in word:
            if (c in vowels):
                count = count + 1                                        
            if (count > 2):                                              
                complex_count = complex_count + 1 
    perc_complex_words = complex_count/no_words                      
    return no_words, complex_count, perc_complex_words

def uncer_constr_calc(x):
    x      = x.translate(str.maketrans('','',string.punctuation)) 
    tokens = nltk.word_tokenize(x)                                
    tokens = [x.lower() for x in tokens]                          

    uncer_words = constr_words = 0

    for word in tokens:                                         
        if word in uncertainity_words:                              
            uncer_words  += 1
        if word in constraining_words:                                              
            constr_words += 1 

    uncer_count  = uncer_words
    constr_count = constr_words

    return uncer_count, constr_count

link = 'https://www.sec.gov/Archives/'
secfname_or = secfname_or.reset_index(drop=True)
file_rec['SECFNAME'] = secfname_or 
file_rec
with open('Final_Submission_File.csv', 'w') as f:
    file_rec.to_csv(f)